{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b17a4fc",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1dfb91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "\n",
    "import os, sys\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "#!pip install six numpy scipy Pillow matplotlib scikit-image opencv-python imageio\n",
    "#!pip install --no-dependencies imgaug\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "WORKERS = 2\n",
    "CHANNEL = 3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "IMG_SIZE = 512\n",
    "NUM_CLASSES = 18\n",
    "SEED = 42\n",
    "TRAIN_NUM = 1000 # use 1000 when you just want to explore new idea, use -1 for full train\n",
    "\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5695d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/opt/ml/code/level1-image-classification-level1-nlp-6/train.csv')\n",
    "df_valid = pd.read_csv('/opt/ml/code/level1-image-classification-level1-nlp-6/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1567e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_class = pd.DataFrame(columns = ['id', 'per_id', 'gender', 'age', 'mask', 'class', 'path'])\n",
    "df_train_class.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f5c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_class_simple(row, mask):\n",
    "    # Assuming the mask is already labeled as 0,1,2\n",
    "    # Each of them is 'wear', 'incorrect' and 'not wear'\n",
    "    gender = 0 if row[\"gender\"] == \"male\" else 3\n",
    "    age = min(2, row[\"age\"] // 30)\n",
    "\n",
    "    # Print the class number\n",
    "    return mask*6 + gender + age, age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd420e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../input/data/train/images'\n",
    "\n",
    "!rm -rf ./data/train/.DS_Store\n",
    "!rm -rf ./data/train/images/.DS_Store\n",
    "folders = sorted([f for f in os.listdir(path) if \"._\" not in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7a278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "mask_dict = {0: 'wear', 1: 'not wear', 2: 'incorrect'}\n",
    "age_dict = {0: 'under 30', 1: '30 to 60', 2: 'over 60'}\n",
    "\n",
    "for i in df_train.index:\n",
    "    row = df_train.loc[i]\n",
    "    imgs_path = os.path.join(path, row['path'])\n",
    "    images = sorted([f for f in os.listdir(imgs_path) if \"._\" not in f])\n",
    "    for img in images:\n",
    "        #print(img)\n",
    "        if img[:-4] == 'incorrect_mask':\n",
    "            mask = 2 # incorrect\n",
    "        elif img[:-4] == 'normal':\n",
    "            mask = 1 # not wear\n",
    "        else:\n",
    "            mask = 0 # wear\n",
    "\n",
    "        classnum, age = return_class_simple(row, mask)\n",
    "        \n",
    "\n",
    "        df_train_class.loc[idx] = [row['id'], row['gender'], age_dict[age], mask_dict[mask], classnum, os.path.join(imgs_path, img)]\n",
    "        idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70a375fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_class.to_csv(\"./train_with_class.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e3e85",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94a731ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/opt/ml/code/level1-image-classification-level1-nlp-6/train.csv')\n",
    "valid_df = pd.read_csv('/opt/ml/code/level1-image-classification-level1-nlp-6/valid.csv')\n",
    "test_df = pd.read_csv('/opt/ml/code/level1-image-classification-level1-nlp-6/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b8e61aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>per_id</th>\n",
       "      <th>class</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>000225</td>\n",
       "      <td>15</td>\n",
       "      <td>../../input/data/train/images/000225_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000225</td>\n",
       "      <td>3</td>\n",
       "      <td>../../input/data/train/images/000225_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000225</td>\n",
       "      <td>3</td>\n",
       "      <td>../../input/data/train/images/000225_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>000225</td>\n",
       "      <td>3</td>\n",
       "      <td>../../input/data/train/images/000225_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>000225</td>\n",
       "      <td>3</td>\n",
       "      <td>../../input/data/train/images/000225_female_As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11286</th>\n",
       "      <td>11286</td>\n",
       "      <td>004096</td>\n",
       "      <td>2</td>\n",
       "      <td>../../input/data/train/images/004096_male_Asia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11287</th>\n",
       "      <td>11287</td>\n",
       "      <td>004096</td>\n",
       "      <td>2</td>\n",
       "      <td>../../input/data/train/images/004096_male_Asia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11288</th>\n",
       "      <td>11288</td>\n",
       "      <td>004096</td>\n",
       "      <td>2</td>\n",
       "      <td>../../input/data/train/images/004096_male_Asia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11289</th>\n",
       "      <td>11289</td>\n",
       "      <td>004096</td>\n",
       "      <td>2</td>\n",
       "      <td>../../input/data/train/images/004096_male_Asia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11290</th>\n",
       "      <td>11290</td>\n",
       "      <td>004096</td>\n",
       "      <td>8</td>\n",
       "      <td>../../input/data/train/images/004096_male_Asia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11291 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  per_id  class  \\\n",
       "0               0  000225     15   \n",
       "1               1  000225      3   \n",
       "2               2  000225      3   \n",
       "3               3  000225      3   \n",
       "4               4  000225      3   \n",
       "...           ...     ...    ...   \n",
       "11286       11286  004096      2   \n",
       "11287       11287  004096      2   \n",
       "11288       11288  004096      2   \n",
       "11289       11289  004096      2   \n",
       "11290       11290  004096      8   \n",
       "\n",
       "                                                    path  \n",
       "0      ../../input/data/train/images/000225_female_As...  \n",
       "1      ../../input/data/train/images/000225_female_As...  \n",
       "2      ../../input/data/train/images/000225_female_As...  \n",
       "3      ../../input/data/train/images/000225_female_As...  \n",
       "4      ../../input/data/train/images/000225_female_As...  \n",
       "...                                                  ...  \n",
       "11286  ../../input/data/train/images/004096_male_Asia...  \n",
       "11287  ../../input/data/train/images/004096_male_Asia...  \n",
       "11288  ../../input/data/train/images/004096_male_Asia...  \n",
       "11289  ../../input/data/train/images/004096_male_Asia...  \n",
       "11290  ../../input/data/train/images/004096_male_Asia...  \n",
       "\n",
       "[11291 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11d7e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출처: https://github.com/utkuozbulak/pytorch-custom-dataset-examples/blob/master/src/custom_dataset_from_file.py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset  # For custom datasets\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "\n",
    "transform = transforms.Compose([Resize((512, 384), Image.BILINEAR),\n",
    "                                ToTensor(),\n",
    "                                Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2))])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df_train, transform, train=True):\n",
    "        # Get image list\n",
    "        self.image_list = df_train['path'].tolist()\n",
    "        self.target = df_train['class'].tolist()\n",
    "        # Calculate len\n",
    "        self.data_len = len(self.image_list)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image name from the pandas df\n",
    "        single_image_path = self.image_list[index]\n",
    "        # Open image\n",
    "        # Open image\n",
    "        image = Image.open(single_image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(image)\n",
    "    \n",
    "        if self.train:\n",
    "            label = self.target[index]\n",
    "            \n",
    "            return (img, torch.tensor(label))\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb2be9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9db19643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "\n",
    "resnet18_pretrained = models.resnet18(pretrained=True)\n",
    "\n",
    "#for para in resnet18_pretrained.parameters():\n",
    "#    para.requires_grad = False\n",
    "    \n",
    "num_classes = 18\n",
    "num_ftrs = resnet18_pretrained.fc.in_features\n",
    "resnet18_pretrained.fc = nn.Linear(num_ftrs, num_classes)\n",
    "#resnet18_pretrained.fc.\n",
    "\n",
    "#print(resnet18_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c331c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7be9bcd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-9a78cceaa4ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyCustomModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-1ac4c2ec3564>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# initialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mstdv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstdv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstdv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "   \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataset import Dataset  # For custom datasets\n",
    "\n",
    "report_every = 100\n",
    "\n",
    "def eval(model,data_iter,criterion,epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    batch_num = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(data_iter):\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        total_loss += loss.data\n",
    "        batch_num += 1\n",
    "        \n",
    "    loss = total_loss / batch_num\n",
    "    model.train()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Dataset variant 3:\n",
    "    # Read images from a folder, image classes are embedded in file names\n",
    "    # No csv is used whatsoever\n",
    "    # No torch transformations are used\n",
    "    # Preprocessing operations are defined inside the dataset\n",
    "    train_dataset = CustomDataset(train_df, transform = transform)\n",
    "    valid_dataset = CustomDataset(valid_df, transform = transform)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                    batch_size=8,\n",
    "                                                    shuffle=False)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                                    batch_size=8,\n",
    "                                                    shuffle=False)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    model = MyCustomModel()\n",
    "    model = model.to(device)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    criterion = FocalLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    running_loss = 0\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(100): \n",
    "        for i, (images, labels) in enumerate(train_dataloader):\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if i % report_every == 0:\n",
    "                eval_loss = eval(model, valid_dataloader, criterion, epoch)\n",
    "                if eval_loss < min_loss:\n",
    "                    min_loss = eval_loss\n",
    "                    torch.save(model, \"./model/epoch_%d_loss_%6f.pt\" % (epoch, min_loss))\n",
    "                print('Epoch: %d - Batch ID:%d - Min Loss:%f' %(epoch, i, min_loss))\n",
    "            \n",
    "\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5cc0b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "module.conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "module.bn1.weight \t torch.Size([64])\n",
      "module.bn1.bias \t torch.Size([64])\n",
      "module.bn1.running_mean \t torch.Size([64])\n",
      "module.bn1.running_var \t torch.Size([64])\n",
      "module.bn1.num_batches_tracked \t torch.Size([])\n",
      "module.layer1.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "module.layer1.0.bn1.weight \t torch.Size([64])\n",
      "module.layer1.0.bn1.bias \t torch.Size([64])\n",
      "module.layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "module.layer1.0.bn1.running_var \t torch.Size([64])\n",
      "module.layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "module.layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "module.layer1.0.bn2.weight \t torch.Size([64])\n",
      "module.layer1.0.bn2.bias \t torch.Size([64])\n",
      "module.layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "module.layer1.0.bn2.running_var \t torch.Size([64])\n",
      "module.layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "module.layer1.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "module.layer1.1.bn1.weight \t torch.Size([64])\n",
      "module.layer1.1.bn1.bias \t torch.Size([64])\n",
      "module.layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "module.layer1.1.bn1.running_var \t torch.Size([64])\n",
      "module.layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "module.layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "module.layer1.1.bn2.weight \t torch.Size([64])\n",
      "module.layer1.1.bn2.bias \t torch.Size([64])\n",
      "module.layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "module.layer1.1.bn2.running_var \t torch.Size([64])\n",
      "module.layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "module.layer2.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "module.layer2.0.bn1.weight \t torch.Size([128])\n",
      "module.layer2.0.bn1.bias \t torch.Size([128])\n",
      "module.layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "module.layer2.0.bn1.running_var \t torch.Size([128])\n",
      "module.layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "module.layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "module.layer2.0.bn2.weight \t torch.Size([128])\n",
      "module.layer2.0.bn2.bias \t torch.Size([128])\n",
      "module.layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "module.layer2.0.bn2.running_var \t torch.Size([128])\n",
      "module.layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "module.layer2.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "module.layer2.0.downsample.1.weight \t torch.Size([128])\n",
      "module.layer2.0.downsample.1.bias \t torch.Size([128])\n",
      "module.layer2.0.downsample.1.running_mean \t torch.Size([128])\n",
      "module.layer2.0.downsample.1.running_var \t torch.Size([128])\n",
      "module.layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "module.layer2.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "module.layer2.1.bn1.weight \t torch.Size([128])\n",
      "module.layer2.1.bn1.bias \t torch.Size([128])\n",
      "module.layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "module.layer2.1.bn1.running_var \t torch.Size([128])\n",
      "module.layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "module.layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "module.layer2.1.bn2.weight \t torch.Size([128])\n",
      "module.layer2.1.bn2.bias \t torch.Size([128])\n",
      "module.layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "module.layer2.1.bn2.running_var \t torch.Size([128])\n",
      "module.layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "module.layer3.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "module.layer3.0.bn1.weight \t torch.Size([256])\n",
      "module.layer3.0.bn1.bias \t torch.Size([256])\n",
      "module.layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "module.layer3.0.bn1.running_var \t torch.Size([256])\n",
      "module.layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "module.layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "module.layer3.0.bn2.weight \t torch.Size([256])\n",
      "module.layer3.0.bn2.bias \t torch.Size([256])\n",
      "module.layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "module.layer3.0.bn2.running_var \t torch.Size([256])\n",
      "module.layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "module.layer3.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "module.layer3.0.downsample.1.weight \t torch.Size([256])\n",
      "module.layer3.0.downsample.1.bias \t torch.Size([256])\n",
      "module.layer3.0.downsample.1.running_mean \t torch.Size([256])\n",
      "module.layer3.0.downsample.1.running_var \t torch.Size([256])\n",
      "module.layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "module.layer3.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "module.layer3.1.bn1.weight \t torch.Size([256])\n",
      "module.layer3.1.bn1.bias \t torch.Size([256])\n",
      "module.layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "module.layer3.1.bn1.running_var \t torch.Size([256])\n",
      "module.layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "module.layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "module.layer3.1.bn2.weight \t torch.Size([256])\n",
      "module.layer3.1.bn2.bias \t torch.Size([256])\n",
      "module.layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "module.layer3.1.bn2.running_var \t torch.Size([256])\n",
      "module.layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "module.layer4.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "module.layer4.0.bn1.weight \t torch.Size([512])\n",
      "module.layer4.0.bn1.bias \t torch.Size([512])\n",
      "module.layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "module.layer4.0.bn1.running_var \t torch.Size([512])\n",
      "module.layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "module.layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "module.layer4.0.bn2.weight \t torch.Size([512])\n",
      "module.layer4.0.bn2.bias \t torch.Size([512])\n",
      "module.layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "module.layer4.0.bn2.running_var \t torch.Size([512])\n",
      "module.layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "module.layer4.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "module.layer4.0.downsample.1.weight \t torch.Size([512])\n",
      "module.layer4.0.downsample.1.bias \t torch.Size([512])\n",
      "module.layer4.0.downsample.1.running_mean \t torch.Size([512])\n",
      "module.layer4.0.downsample.1.running_var \t torch.Size([512])\n",
      "module.layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "module.layer4.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "module.layer4.1.bn1.weight \t torch.Size([512])\n",
      "module.layer4.1.bn1.bias \t torch.Size([512])\n",
      "module.layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "module.layer4.1.bn1.running_var \t torch.Size([512])\n",
      "module.layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "module.layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "module.layer4.1.bn2.weight \t torch.Size([512])\n",
      "module.layer4.1.bn2.bias \t torch.Size([512])\n",
      "module.layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "module.layer4.1.bn2.running_var \t torch.Size([512])\n",
      "module.layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "module.fc.weight \t torch.Size([18, 512])\n",
      "module.fc.bias \t torch.Size([18])\n"
     ]
    }
   ],
   "source": [
    "# 모델의 state_dict 출력\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "  \n",
    "torch.save(model, \"./model/baseline_resnet+focal_loss.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1fde02",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e09246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./model/epoch_16_loss_3.823638.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c879a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_df, transform)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=8,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ea463a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "all_predictions = []\n",
    "for images, labels in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        #images = images.to(device)\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a846683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.2153558052434457\n",
      "f1 0.0689292044797463\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "print('accuracy', metrics.accuracy_score(targets, all_predictions) )\n",
    "print('f1', np.mean(metrics.f1_score(targets, all_predictions, average=None)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7b062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a33fd88",
   "metadata": {},
   "source": [
    "# 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2cc4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/opt/ml/input/data/train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6356209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['age_class'] = 0\n",
    "\n",
    "# 30 미만: 0 / 30 ~ 60: 1 / 60 이상: 2\n",
    "df_train.loc[df_train['age'] < 30, 'age_class'] = 0\n",
    "df_train.loc[(df_train['age'] < 60) & (df_train['age'] >= 30), 'age_class'] = 1\n",
    "df_train.loc[df_train['age'] >= 60, 'age_class'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d705345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['race'], axis=1)\n",
    "df_train = df_train.drop(['age'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3c2046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sort_values(by=['gender', 'age_class'])\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_train['index'] = df_train.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86f3c9",
   "metadata": {},
   "source": [
    "* 여성 0 ~ 1657\n",
    "    * under 30: 0 ~ 731\n",
    "        * train: 0 ~ 438\n",
    "        * valid: 439 ~ 585\n",
    "        * test: 586 ~ 731\n",
    "    * 30 to 60: 732 ~ 1548\n",
    "        * train: 732 ~ 1221\n",
    "        * valid: 1222 ~ 1384\n",
    "        * test: 1385 ~ 1548\n",
    "    * over 60: 1549 ~ 1657\n",
    "        * train: 1549 ~ 1613\n",
    "        * valid: 1614 ~ 1635\n",
    "        * test: 1636 ~ 1657\n",
    "* 남성 1658 ~ 2699\n",
    "    * under 30: 1658 ~ 2206\n",
    "        * train: 1658 ~ 1986\n",
    "        * valid: 1987 ~ 2097\n",
    "        * test: 2098 ~ 2206\n",
    "    * 30 to 60: 2207 ~ 2616\n",
    "        * train: 2207 ~ 2452\n",
    "        * valid: 2453 ~ 2534\n",
    "        * test: 2535 ~ 2616\n",
    "    * over 60: 2617 ~ 2699\n",
    "        * train: 2617 ~ 2666\n",
    "        * valid: 2667 ~ 2682\n",
    "        * test: 2683 ~ 2699\n",
    "\n",
    "* train 60% / valid 20% / test 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c641eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([df_train[0:438], df_train[732:1221], df_train[1549:1613], df_train[1658:1986], df_train[2207:2452], df_train[2617:2666]])\n",
    "valid_df = pd.concat([df_train[429:585], df_train[1222:1384], df_train[1614:1635], df_train[1987:2097], df_train[2453:2534], df_train[2667:2682]])\n",
    "test_df = pd.concat([df_train[586:731], df_train[1385:1548], df_train[1636:1657], df_train[2098:2206], df_train[2535:2616], df_train[2683:2699]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c75812e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "      <th>age_class</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>003326</td>\n",
       "      <td>female</td>\n",
       "      <td>003326_female_Asian_20</td>\n",
       "      <td>0</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>003328</td>\n",
       "      <td>female</td>\n",
       "      <td>003328_female_Asian_19</td>\n",
       "      <td>0</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>003329</td>\n",
       "      <td>female</td>\n",
       "      <td>003329_female_Asian_19</td>\n",
       "      <td>0</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>003330</td>\n",
       "      <td>female</td>\n",
       "      <td>003330_female_Asian_19</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>003331</td>\n",
       "      <td>female</td>\n",
       "      <td>003331_female_Asian_20</td>\n",
       "      <td>0</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2677</th>\n",
       "      <td>004283</td>\n",
       "      <td>male</td>\n",
       "      <td>004283_male_Asian_60</td>\n",
       "      <td>2</td>\n",
       "      <td>2677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678</th>\n",
       "      <td>004284</td>\n",
       "      <td>male</td>\n",
       "      <td>004284_male_Asian_60</td>\n",
       "      <td>2</td>\n",
       "      <td>2678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2679</th>\n",
       "      <td>004285</td>\n",
       "      <td>male</td>\n",
       "      <td>004285_male_Asian_60</td>\n",
       "      <td>2</td>\n",
       "      <td>2679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680</th>\n",
       "      <td>004286</td>\n",
       "      <td>male</td>\n",
       "      <td>004286_male_Asian_60</td>\n",
       "      <td>2</td>\n",
       "      <td>2680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2681</th>\n",
       "      <td>004289</td>\n",
       "      <td>male</td>\n",
       "      <td>004289_male_Asian_60</td>\n",
       "      <td>2</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>545 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender                    path  age_class  index\n",
       "429   003326  female  003326_female_Asian_20          0    429\n",
       "430   003328  female  003328_female_Asian_19          0    430\n",
       "431   003329  female  003329_female_Asian_19          0    431\n",
       "432   003330  female  003330_female_Asian_19          0    432\n",
       "433   003331  female  003331_female_Asian_20          0    433\n",
       "...      ...     ...                     ...        ...    ...\n",
       "2677  004283    male    004283_male_Asian_60          2   2677\n",
       "2678  004284    male    004284_male_Asian_60          2   2678\n",
       "2679  004285    male    004285_male_Asian_60          2   2679\n",
       "2680  004286    male    004286_male_Asian_60          2   2680\n",
       "2681  004289    male    004289_male_Asian_60          2   2681\n",
       "\n",
       "[545 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64f65144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_class = pd.DataFrame(columns = ['per_id', 'class', 'path'])\n",
    "#df_train_class.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1db174bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_class_simple(row, mask):\n",
    "    # Assuming the mask is already labeled as 0,1,2\n",
    "    # Each of them is 'wear', 'incorrect' and 'not wear'\n",
    "    gender = 0 if row[\"gender\"] == \"male\" else 3\n",
    "    age = row[\"age_class\"]\n",
    "\n",
    "    # Print the class number\n",
    "    return mask*6 + gender + age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c7bcb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../input/data/train/images'\n",
    "idx = 0\n",
    "mask_dict = {0: 'wear', 1: 'not wear', 2: 'incorrect'}\n",
    "age_dict = {0: 'under 30', 1: '30 to 60', 2: 'over 60'}\n",
    "\n",
    "for i in train_df.index:\n",
    "    row = train_df.loc[i]\n",
    "\n",
    "    \n",
    "    imgs_path = os.path.join(path, row['path'])\n",
    "    images = sorted([f for f in os.listdir(imgs_path) if \"._\" not in f])\n",
    "    for img in images:\n",
    "        if img[:-4] == 'incorrect_mask':\n",
    "            mask = 2 # incorrect\n",
    "        elif img[:-4] == 'normal':\n",
    "            mask = 1 # not wear\n",
    "        else:\n",
    "            mask = 0 # wear\n",
    "\n",
    "        classnum = return_class_simple(row, mask)\n",
    "        \n",
    "\n",
    "        df_train_class.loc[idx] = [row['id'], classnum, os.path.join(imgs_path, img)]\n",
    "        idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fe18c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_class = pd.DataFrame(columns = ['per_id', 'class', 'path'])\n",
    "#df_train_class.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d67bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../input/data/train/images'\n",
    "idx = 0\n",
    "mask_dict = {0: 'wear', 1: 'not wear', 2: 'incorrect'}\n",
    "age_dict = {0: 'under 30', 1: '30 to 60', 2: 'over 60'}\n",
    "\n",
    "for i in valid_df.index:\n",
    "    row = valid_df.loc[i]\n",
    "\n",
    "    \n",
    "    imgs_path = os.path.join(path, row['path'])\n",
    "    images = sorted([f for f in os.listdir(imgs_path) if \"._\" not in f])\n",
    "    for img in images:\n",
    "        if img[:-4] == 'incorrect_mask':\n",
    "            mask = 2 # incorrect\n",
    "        elif img[:-4] == 'normal':\n",
    "            mask = 1 # not wear\n",
    "        else:\n",
    "            mask = 0 # wear\n",
    "\n",
    "        classnum = return_class_simple(row, mask)\n",
    "        \n",
    "\n",
    "        df_valid_class.loc[idx] = [row['id'], classnum, os.path.join(imgs_path, img)]\n",
    "        idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44010654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_class = pd.DataFrame(columns = ['per_id', 'class', 'path'])\n",
    "#df_train_class.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afd2bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../input/data/train/images'\n",
    "idx = 0\n",
    "mask_dict = {0: 'wear', 1: 'not wear', 2: 'incorrect'}\n",
    "age_dict = {0: 'under 30', 1: '30 to 60', 2: 'over 60'}\n",
    "\n",
    "for i in test_df.index:\n",
    "    row = test_df.loc[i]\n",
    "\n",
    "    \n",
    "    imgs_path = os.path.join(path, row['path'])\n",
    "    images = sorted([f for f in os.listdir(imgs_path) if \"._\" not in f])\n",
    "    for img in images:\n",
    "        if img[:-4] == 'incorrect_mask':\n",
    "            mask = 2 # incorrect\n",
    "        elif img[:-4] == 'normal':\n",
    "            mask = 1 # not wear\n",
    "        else:\n",
    "            mask = 0 # wear\n",
    "\n",
    "        classnum = return_class_simple(row, mask)\n",
    "        \n",
    "\n",
    "        df_test_class.loc[idx] = [row['id'], classnum, os.path.join(imgs_path, img)]\n",
    "        idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca2a5c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_class.to_csv(\"./train.csv\")\n",
    "df_valid_class.to_csv(\"./valid.csv\")\n",
    "df_test_class.to_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d5d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
