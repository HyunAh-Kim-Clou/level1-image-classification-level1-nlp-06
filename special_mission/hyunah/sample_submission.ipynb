{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from loss import F1Loss, FocalLoss\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torchvision.transforms.functional import to_pil_image # tensor to pil_image\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 폴더 경로를 지정\n",
    "train_dir = '../../../../input/data/train'\n",
    "test_dir = '../../../../input/data/eval'\n",
    "model_dir = '../../../../code/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af26a58b-6b25-421c-bdbb-878408369fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id gender   race  age                  path\n",
       "2697  006956   male  Asian   19  006956_male_Asian_19\n",
       "2698  006957   male  Asian   20  006957_male_Asian_20\n",
       "2699  006959   male  Asian   19  006959_male_Asian_19"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "train_csv.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9259218c-0cc8-401d-90ec-5b43108633b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe41d61c-cf8d-4826-a86b-858ad95af023",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_gender = lambda x: int(x == \"female\")\n",
    "mapping_age = lambda x: min(2, x // 30)\n",
    "mapping_mask = lambda x: x.startswith('mask') and 0 or (x.startswith('incorrect') and 1 or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15356b59-0c77-4136-a4f4-2430dbac4917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>2</td>\n",
       "      <td>../../../../input/data/train/images/006959_mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>1</td>\n",
       "      <td>../../../../input/data/train/images/006959_mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>2</td>\n",
       "      <td>../../../../input/data/train/images/006959_mal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y                                               path\n",
       "18897  2  ../../../../input/data/train/images/006959_mal...\n",
       "18898  1  ../../../../input/data/train/images/006959_mal...\n",
       "18899  2  ../../../../input/data/train/images/006959_mal..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-processing\n",
    "# path to image_path 변경\n",
    "image_dir = os.path.join(train_dir, 'images')\n",
    "\n",
    "train_meta = pd.DataFrame()\n",
    "for i in range(len(train_csv)):\n",
    "    _, gender, _, age, path = train_csv.iloc[i]\n",
    "    image_path = os.path.join(image_dir, path)\n",
    "\n",
    "    li = []\n",
    "    for f in os.listdir(image_path):\n",
    "        if not f.startswith('.'):\n",
    "            target = 6*mapping_gender(gender) + 3*mapping_age(age) + mapping_mask(f)\n",
    "            li.append((target, os.path.join(image_path, f)))\n",
    "    li = pd.DataFrame({name: data for name, data in  zip(['y', 'path'], zip(*li))})    \n",
    "    train_meta = train_meta.append(li, ignore_index = True) # 뒤에 계속 합쳐두기\n",
    "train_meta.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb222d1c-f439-4d6c-ac67-4dcfdc46c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d22f6540-e1d0-4de7-901a-2f894ccac0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15120,) (15120,)\n",
      "(3780,) (3780,)\n"
     ]
    }
   ],
   "source": [
    "# train, test dataset 나누기\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "        train_meta.path.to_numpy(), \n",
    "        train_meta.y.to_numpy(), \n",
    "        test_size=0.2, shuffle=False)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_eval.shape, y_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8b1967b-b1fc-49bf-b5d0-7e411b31265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    # input: image_list, target_list\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.image_paths = X\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    # output: PIL_image, label\n",
    "    def __getitem__(self, index):\n",
    "        images = []\n",
    "        image = Image.open(self.image_paths[index])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return (image, self.target[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa145709-f1fd-4f18-8db2-5f87020b5faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15120, 3780)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = TrainDataset(X_train, y_train, transform)\n",
    "eval_ds = TrainDataset(X_eval, y_eval, transform)\n",
    "\n",
    "len(train_ds), len(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a8a074-3c42-4943-b678-ba0123e9e3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15120, 3784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "len(train_loader)*BATCH_SIZE, len(eval_loader)*BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5002f32-9f05-4c3c-97c0-af625e5044b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "def show_images(data, n = 5, rows=1, cols=7):\n",
    "    plt.figure(figsize=(20,14))\n",
    "\n",
    "    k = 1\n",
    "    for im, *ans in data[:n]:\n",
    "        plt.subplot(rows, cols, k)\n",
    "        plt.imshow(to_pil_image(im))\n",
    "        plt.title(ans, fontsize = 16)\n",
    "        plt.axis('off')\n",
    "        k += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a451e7f5-329f-4fa3-9d99-09d23013ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = []\n",
    "# for i in range(1):\n",
    "#     data = train_loader\n",
    "#     images.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd199acc-449d-41c9-a406-928319dbbddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,4))\n",
    "\n",
    "# for i, x in enumerate(['mask', 'gender', 'age', 'y']):\n",
    "#     data = train_meta[x].value_counts()\n",
    "#     plt.subplot(1, 4, i+1)\n",
    "#     plt.bar(data.index, data.values, tick_label=data.index)\n",
    "#     plt.title(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-organizer",
   "metadata": {},
   "source": [
    "## 1. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f441ea-5711-4e5a-b695-58f3f6db009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "model = models.resnet34(pretrained=True).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optm = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "num_classes = 18\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes).to(device)\n",
    "\n",
    "# for x in model.children():\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a58ea8ac-4120-4e19-a847-823605bf0a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Loss: 1.3554\n",
      "epoch: 0 Loss: 2.5032\n",
      "epoch: 0 Loss: 1.4187\n",
      "epoch: 0 Loss: 2.3878\n",
      "epoch: 0 Loss: 2.2375\n",
      "epoch: 0 Loss: 1.3378\n",
      "epoch: 0 Loss: 1.7886\n",
      "epoch: 0 Loss: 1.1882\n",
      "epoch: 0 Loss: 1.2984\n",
      "epoch: 0 Loss: 1.0064\n",
      "epoch: 0 Loss: 2.0053\n",
      "epoch: 0 Loss: 0.8394\n",
      "epoch: 0 Loss: 1.0736\n",
      "epoch: 0 Loss: 1.4622\n",
      "epoch: 0 Loss: 1.1537\n",
      "epoch: 0 Loss: 1.4220\n",
      "epoch: 0 Loss: 2.4918\n",
      "epoch: 0 Loss: 0.9883\n",
      "epoch: 0 Loss: 1.4397\n",
      "epoch: 0 Loss: 1.0044\n",
      "epoch: 0 Loss: 1.2077\n",
      "epoch: 0 Loss: 1.6809\n",
      "epoch: 0 Loss: 1.5316\n",
      "epoch: 0 Loss: 2.5592\n",
      "epoch: 0 Loss: 1.0840\n",
      "epoch: 0 Loss: 1.0213\n",
      "epoch: 0 Loss: 1.9748\n",
      "epoch: 0 Loss: 1.5413\n",
      "epoch: 0 Loss: 2.3123\n",
      "epoch: 0 Loss: 1.8131\n",
      "epoch: 0 Loss: 1.3540\n",
      "epoch: 0 Loss: 1.7792\n",
      "epoch: 0 Loss: 0.8160\n",
      "epoch: 0 Loss: 1.4878\n",
      "epoch: 0 Loss: 2.0312\n",
      "epoch: 0 Loss: 1.6843\n",
      "epoch: 0 Loss: 1.7600\n",
      "epoch: 0 Loss: 1.0868\n",
      "epoch: 0 Loss: 1.5406\n",
      "epoch: 0 Loss: 1.4920\n",
      "epoch: 0 Loss: 1.9743\n",
      "epoch: 0 Loss: 2.0738\n",
      "epoch: 0 Loss: 1.7415\n",
      "epoch: 0 Loss: 1.0732\n",
      "epoch: 0 Loss: 1.2194\n",
      "epoch: 0 Loss: 2.1284\n",
      "epoch: 0 Loss: 1.5256\n",
      "epoch: 0 Loss: 0.9396\n",
      "epoch: 0 Loss: 1.3419\n",
      "epoch: 0 Loss: 0.5938\n",
      "epoch: 0 Loss: 0.8876\n",
      "epoch: 0 Loss: 1.2424\n",
      "epoch: 0 Loss: 0.7142\n",
      "epoch: 0 Loss: 1.6404\n",
      "epoch: 0 Loss: 1.0066\n",
      "epoch: 0 Loss: 2.2896\n",
      "epoch: 0 Loss: 1.2637\n",
      "epoch: 0 Loss: 1.1346\n",
      "epoch: 0 Loss: 1.1106\n",
      "epoch: 0 Loss: 1.8734\n",
      "epoch: 0 Loss: 1.1394\n",
      "epoch: 0 Loss: 0.7764\n",
      "epoch: 0 Loss: 1.0089\n",
      "epoch: 1 Loss: 1.5994\n",
      "epoch: 1 Loss: 1.9767\n",
      "epoch: 1 Loss: 1.0927\n",
      "epoch: 1 Loss: 0.8781\n",
      "epoch: 1 Loss: 0.6172\n",
      "epoch: 1 Loss: 0.8069\n",
      "epoch: 1 Loss: 1.8151\n",
      "epoch: 1 Loss: 1.3638\n",
      "epoch: 1 Loss: 1.5642\n",
      "epoch: 1 Loss: 1.5744\n",
      "epoch: 1 Loss: 0.4662\n",
      "epoch: 1 Loss: 0.8232\n",
      "epoch: 1 Loss: 1.6685\n",
      "epoch: 1 Loss: 0.8661\n",
      "epoch: 1 Loss: 0.8206\n",
      "epoch: 1 Loss: 0.7847\n",
      "epoch: 1 Loss: 0.9782\n",
      "epoch: 1 Loss: 0.9670\n",
      "epoch: 1 Loss: 1.4471\n",
      "epoch: 1 Loss: 1.5228\n",
      "epoch: 1 Loss: 0.5580\n",
      "epoch: 1 Loss: 0.8719\n",
      "epoch: 1 Loss: 1.0045\n",
      "epoch: 1 Loss: 0.7294\n",
      "epoch: 1 Loss: 1.4514\n",
      "epoch: 1 Loss: 0.7211\n",
      "epoch: 1 Loss: 0.6740\n",
      "epoch: 1 Loss: 1.1042\n",
      "epoch: 1 Loss: 0.8579\n",
      "epoch: 1 Loss: 0.7138\n",
      "epoch: 1 Loss: 0.9655\n",
      "epoch: 1 Loss: 0.7303\n",
      "epoch: 1 Loss: 0.3894\n",
      "epoch: 1 Loss: 0.5428\n",
      "epoch: 1 Loss: 0.5177\n",
      "epoch: 1 Loss: 1.3566\n",
      "epoch: 1 Loss: 0.7299\n",
      "epoch: 1 Loss: 0.8943\n",
      "epoch: 1 Loss: 0.6250\n",
      "epoch: 1 Loss: 1.1968\n",
      "epoch: 1 Loss: 0.6912\n",
      "epoch: 1 Loss: 0.7983\n",
      "epoch: 1 Loss: 0.2951\n",
      "epoch: 1 Loss: 0.9390\n",
      "epoch: 1 Loss: 0.6180\n",
      "epoch: 1 Loss: 0.9817\n",
      "epoch: 1 Loss: 0.7649\n",
      "epoch: 1 Loss: 0.8166\n",
      "epoch: 1 Loss: 0.7320\n",
      "epoch: 1 Loss: 0.4810\n",
      "epoch: 1 Loss: 0.6342\n",
      "epoch: 1 Loss: 0.7836\n",
      "epoch: 1 Loss: 0.9172\n",
      "epoch: 1 Loss: 0.4414\n",
      "epoch: 1 Loss: 0.7887\n",
      "epoch: 1 Loss: 0.7025\n",
      "epoch: 1 Loss: 1.1922\n",
      "epoch: 1 Loss: 1.0564\n",
      "epoch: 1 Loss: 0.5494\n",
      "epoch: 1 Loss: 0.8623\n",
      "epoch: 1 Loss: 0.3525\n",
      "epoch: 1 Loss: 0.4351\n",
      "epoch: 1 Loss: 0.6615\n",
      "epoch: 2 Loss: 0.9794\n",
      "epoch: 2 Loss: 0.8116\n",
      "epoch: 2 Loss: 0.9738\n",
      "epoch: 2 Loss: 0.6684\n",
      "epoch: 2 Loss: 1.1442\n",
      "epoch: 2 Loss: 0.5837\n",
      "epoch: 2 Loss: 0.7588\n",
      "epoch: 2 Loss: 0.6371\n",
      "epoch: 2 Loss: 1.2680\n",
      "epoch: 2 Loss: 0.5439\n",
      "epoch: 2 Loss: 0.5685\n",
      "epoch: 2 Loss: 0.5785\n",
      "epoch: 2 Loss: 0.5330\n",
      "epoch: 2 Loss: 0.8411\n",
      "epoch: 2 Loss: 0.6413\n",
      "epoch: 2 Loss: 0.6815\n",
      "epoch: 2 Loss: 0.4655\n",
      "epoch: 2 Loss: 0.6637\n",
      "epoch: 2 Loss: 0.9939\n",
      "epoch: 2 Loss: 0.9123\n",
      "epoch: 2 Loss: 0.5004\n",
      "epoch: 2 Loss: 0.2308\n",
      "epoch: 2 Loss: 0.2995\n",
      "epoch: 2 Loss: 0.4073\n",
      "epoch: 2 Loss: 0.6773\n",
      "epoch: 2 Loss: 0.6552\n",
      "epoch: 2 Loss: 0.5011\n",
      "epoch: 2 Loss: 1.0030\n",
      "epoch: 2 Loss: 0.6873\n",
      "epoch: 2 Loss: 1.8244\n",
      "epoch: 2 Loss: 0.4958\n",
      "epoch: 2 Loss: 0.5486\n",
      "epoch: 2 Loss: 0.9602\n",
      "epoch: 2 Loss: 0.9943\n",
      "epoch: 2 Loss: 0.8403\n",
      "epoch: 2 Loss: 0.4520\n",
      "epoch: 2 Loss: 0.8840\n",
      "epoch: 2 Loss: 0.3805\n",
      "epoch: 2 Loss: 0.5126\n",
      "epoch: 2 Loss: 1.2203\n",
      "epoch: 2 Loss: 0.9517\n",
      "epoch: 2 Loss: 0.6449\n",
      "epoch: 2 Loss: 0.9854\n",
      "epoch: 2 Loss: 0.4452\n",
      "epoch: 2 Loss: 0.2800\n",
      "epoch: 2 Loss: 0.9378\n",
      "epoch: 2 Loss: 0.6360\n",
      "epoch: 2 Loss: 0.6260\n",
      "epoch: 2 Loss: 0.8232\n",
      "epoch: 2 Loss: 0.5519\n",
      "epoch: 2 Loss: 0.3215\n",
      "epoch: 2 Loss: 0.7240\n",
      "epoch: 2 Loss: 0.2644\n",
      "epoch: 2 Loss: 1.1388\n",
      "epoch: 2 Loss: 0.6672\n",
      "epoch: 2 Loss: 0.9532\n",
      "epoch: 2 Loss: 0.7552\n",
      "epoch: 2 Loss: 1.0269\n",
      "epoch: 2 Loss: 1.4743\n",
      "epoch: 2 Loss: 0.4153\n",
      "epoch: 2 Loss: 1.4630\n",
      "epoch: 2 Loss: 1.5361\n",
      "epoch: 2 Loss: 0.7882\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(3):\n",
    "    # image shape : [8, 3, 512, 384]\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs = Variable(imgs).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        \n",
    "        y_pred = model(imgs)\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "\n",
    "        optm.zero_grad()\n",
    "        loss.backward()\n",
    "        optm.step()\n",
    "        \n",
    "        if i % 30 == 0:\n",
    "            print(\"epoch: {} Loss: {:.4f}\".format(epoch, loss.data))\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "471690ac-bcda-4196-a817-461e8b66e38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "start_idx = len(train_ds)\n",
    "\n",
    "targets = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, (imgs, labels) in enumerate(eval_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        y_pred = model(imgs).argmax(dim=-1)\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(y_pred.cpu().numpy())\n",
    "#         print(y_pred, labels)\n",
    "#         tensor([8, 2, 2, 2, 8, 2, 8, 7], device='cuda:0') tensor([11,  2,  1,  2,  8,  2,  8,  7], device='cuda:0')\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61b154a9-9588-4c19-a40f-723afa1c63d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7021164021164021\n",
      "F1 Loss: 0.4136\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format( accuracy_score(targets, all_preds)))\n",
    "print(\"F1 Loss: {:.4f}\".format( np.mean(f1_score(targets, all_preds, average=None)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 2. Test Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02fabd4b-d685-489c-84cc-8a475db9c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, os.path.join(model_dir, \"resnet34_crossentropy_adam.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = torch.load(os.path.join(model_dir, \"resnet34_crossentropy_adam.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "# submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "submission.to_csv(os.path.join(test_dir, 'resnet34_crossentropy_adam.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e90567-d3e2-46e0-9792-aa3d7cdbdbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
