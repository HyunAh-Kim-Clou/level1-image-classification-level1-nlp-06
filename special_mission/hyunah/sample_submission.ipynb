{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from dataset import TrainDataset\n",
    "from loss import F1Loss, FocalLoss\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torchvision.transforms.functional import to_pil_image # tensor to pil_image\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 폴더 경로를 지정\n",
    "train_dir = '../../../../input/data/train'\n",
    "test_dir = '../../../../input/data/eval'\n",
    "model_dir = '../../../../code/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af26a58b-6b25-421c-bdbb-878408369fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id gender   race  age                  path\n",
       "2697  006956   male  Asian   19  006956_male_Asian_19\n",
       "2698  006957   male  Asian   20  006957_male_Asian_20\n",
       "2699  006959   male  Asian   19  006959_male_Asian_19"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "train_csv.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9259218c-0cc8-401d-90ec-5b43108633b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe41d61c-cf8d-4826-a86b-858ad95af023",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_gender = lambda x: int(x == \"female\")\n",
    "mapping_age = lambda x: min(2, x // 30)\n",
    "mapping_mask = lambda x: x.startswith('mask') and 0 or (x.startswith('incorrect') and 1 or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15356b59-0c77-4136-a4f4-2430dbac4917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>2</td>\n",
       "      <td>../../../../input/data/train/images/006959_mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>1</td>\n",
       "      <td>../../../../input/data/train/images/006959_mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>2</td>\n",
       "      <td>../../../../input/data/train/images/006959_mal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y                                               path\n",
       "18897  2  ../../../../input/data/train/images/006959_mal...\n",
       "18898  1  ../../../../input/data/train/images/006959_mal...\n",
       "18899  2  ../../../../input/data/train/images/006959_mal..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-processing\n",
    "# path to image_path 변경\n",
    "image_dir = os.path.join(train_dir, 'images')\n",
    "\n",
    "train_meta = pd.DataFrame()\n",
    "for i in range(len(train_csv)):\n",
    "    _, gender, _, age, path = train_csv.iloc[i]\n",
    "    image_path = os.path.join(image_dir, path)\n",
    "\n",
    "    li = []\n",
    "    for f in os.listdir(image_path):\n",
    "        if not f.startswith('.'):\n",
    "            target = 6*mapping_gender(gender) + 3*mapping_age(age) + mapping_mask(f)\n",
    "            li.append((target, os.path.join(image_path, f)))\n",
    "    li = pd.DataFrame({name: data for name, data in  zip(['y', 'path'], zip(*li))})    \n",
    "    train_meta = train_meta.append(li, ignore_index = True) # 뒤에 계속 합쳐두기\n",
    "train_meta.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb222d1c-f439-4d6c-ac67-4dcfdc46c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d22f6540-e1d0-4de7-901a-2f894ccac0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15120,) (15120,)\n",
      "(3780,) (3780,)\n"
     ]
    }
   ],
   "source": [
    "# train, test dataset 나누기\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "        train_meta.path.to_numpy(), \n",
    "        train_meta.y.to_numpy(), \n",
    "        test_size=0.2, shuffle=False)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_eval.shape, y_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b1967b-b1fc-49bf-b5d0-7e411b31265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    # input: image_list, target_list\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.image_paths = X\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    # output: PIL_image, label\n",
    "    def __getitem__(self, index):\n",
    "        images = []\n",
    "        image = Image.open(self.image_paths[index])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return (image, self.target[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa145709-f1fd-4f18-8db2-5f87020b5faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15120, 3780)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = TrainDataset(X_train, y_train, transform)\n",
    "eval_ds = TrainDataset(X_eval, y_eval, transform)\n",
    "\n",
    "len(train_ds), len(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04a8a074-3c42-4943-b678-ba0123e9e3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15120, 3784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "len(train_loader)*BATCH_SIZE, len(eval_loader)*BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5002f32-9f05-4c3c-97c0-af625e5044b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "def show_images(path, label='', n = 5, rows=1, cols=7):\n",
    "    plt.figure(figsize=(20,14))\n",
    "\n",
    "    k = 1\n",
    "    for im, *ans in data[:n]:\n",
    "        plt.subplot(rows, cols, k)\n",
    "        plt.imshow(im)\n",
    "        plt.title(label, fontsize = 16)\n",
    "        plt.axis('off')\n",
    "        k += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a451e7f5-329f-4fa3-9d99-09d23013ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "for i in range(1200, 1210):\n",
    "    img, label = train_ds[i]\n",
    "    images.append(img)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd199acc-449d-41c9-a406-928319dbbddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,4))\n",
    "\n",
    "# for i, x in enumerate(['mask', 'gender', 'age', 'y']):\n",
    "#     data = train_meta[x].value_counts()\n",
    "#     plt.subplot(1, 4, i+1)\n",
    "#     plt.bar(data.index, data.values, tick_label=data.index)\n",
    "#     plt.title(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-organizer",
   "metadata": {},
   "source": [
    "## 1. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6f441ea-5711-4e5a-b695-58f3f6db009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "model = models.resnet34(pretrained=True).to(device)\n",
    "loss_fn = F1Loss(18) # nn.CrossEntropyLoss()\n",
    "optm = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "num_classes = 18\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes).to(device)\n",
    "\n",
    "# for x in model.children():\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a58ea8ac-4120-4e19-a847-823605bf0a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Loss: 0.9268\n",
      "epoch: 0 Loss: 0.9925\n",
      "epoch: 0 Loss: 0.9305\n",
      "epoch: 0 Loss: 0.8969\n",
      "epoch: 0 Loss: 0.9732\n",
      "epoch: 0 Loss: 0.9562\n",
      "epoch: 0 Loss: 0.9599\n",
      "epoch: 0 Loss: 0.9272\n",
      "epoch: 0 Loss: 0.9780\n",
      "epoch: 0 Loss: 0.9636\n",
      "epoch: 0 Loss: 0.9475\n",
      "epoch: 0 Loss: 0.9537\n",
      "epoch: 0 Loss: 0.9833\n",
      "epoch: 0 Loss: 0.9317\n",
      "epoch: 0 Loss: 0.9105\n",
      "epoch: 0 Loss: 0.8587\n",
      "epoch: 0 Loss: 0.9678\n",
      "epoch: 0 Loss: 0.9018\n",
      "epoch: 0 Loss: 0.9350\n",
      "epoch: 1 Loss: 0.9191\n",
      "epoch: 1 Loss: 0.9100\n",
      "epoch: 1 Loss: 0.9499\n",
      "epoch: 1 Loss: 0.8991\n",
      "epoch: 1 Loss: 0.9121\n",
      "epoch: 1 Loss: 0.9660\n",
      "epoch: 1 Loss: 0.8958\n",
      "epoch: 1 Loss: 0.8789\n",
      "epoch: 1 Loss: 0.8908\n",
      "epoch: 1 Loss: 0.9311\n",
      "epoch: 1 Loss: 0.9121\n",
      "epoch: 1 Loss: 0.9260\n",
      "epoch: 1 Loss: 0.9091\n",
      "epoch: 1 Loss: 0.8799\n",
      "epoch: 1 Loss: 0.8205\n",
      "epoch: 1 Loss: 0.8533\n",
      "epoch: 1 Loss: 0.8693\n",
      "epoch: 1 Loss: 0.8925\n",
      "epoch: 1 Loss: 0.8811\n",
      "epoch: 2 Loss: 0.8951\n",
      "epoch: 2 Loss: 0.8699\n",
      "epoch: 2 Loss: 0.9135\n",
      "epoch: 2 Loss: 0.9687\n",
      "epoch: 2 Loss: 0.8561\n",
      "epoch: 2 Loss: 0.8692\n",
      "epoch: 2 Loss: 0.8822\n",
      "epoch: 2 Loss: 0.8806\n",
      "epoch: 2 Loss: 0.8247\n",
      "epoch: 2 Loss: 0.8939\n",
      "epoch: 2 Loss: 0.9151\n",
      "epoch: 2 Loss: 0.8048\n",
      "epoch: 2 Loss: 0.8855\n",
      "epoch: 2 Loss: 0.8302\n",
      "epoch: 2 Loss: 0.8792\n",
      "epoch: 2 Loss: 0.9238\n",
      "epoch: 2 Loss: 0.9275\n",
      "epoch: 2 Loss: 0.9566\n",
      "epoch: 2 Loss: 0.7932\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(3):\n",
    "    # image shape : [8, 3, 512, 384]\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs = Variable(imgs).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        \n",
    "        y_pred = model(imgs)\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "\n",
    "        optm.zero_grad()\n",
    "        loss.backward()\n",
    "        optm.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch: {} Loss: {:.4f}\".format(epoch, loss.data))\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "471690ac-bcda-4196-a817-461e8b66e38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "start_idx = len(train_ds)\n",
    "\n",
    "targets = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, (imgs, labels) in enumerate(eval_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        y_pred = model(imgs).argmax(dim=-1)\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(y_pred.cpu().numpy())\n",
    "#         print(y_pred, labels)\n",
    "#         tensor([8, 2, 2, 2, 8, 2, 8, 7], device='cuda:0') tensor([11,  2,  1,  2,  8,  2,  8,  7], device='cuda:0')\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61b154a9-9588-4c19-a40f-723afa1c63d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5794\n",
      "F1 Loss: 0.2043\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.4f}\".format( accuracy_score(targets, all_preds)) )\n",
    "print(\"F1 Loss: {:.4f}\".format( np.mean(f1_score(targets, all_preds, average=None)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 2. Test Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fabd4b-d685-489c-84cc-8a475db9c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, os.path.join(model_dir, \"resnet34_crossentropy_adam.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = torch.load(os.path.join(model_dir, \"resnet34_crossentropy_adam.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "# submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "submission.to_csv(os.path.join(test_dir, 'resnet34_crossentropy_adam.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e90567-d3e2-46e0-9792-aa3d7cdbdbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
