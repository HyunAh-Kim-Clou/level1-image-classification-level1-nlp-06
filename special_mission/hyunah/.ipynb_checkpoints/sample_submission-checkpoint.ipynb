{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from loss import F1Loss, FocalLoss\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torchvision.transforms.functional import to_pil_image # tensor to pil_image\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 폴더 경로를 지정\n",
    "train_dir = '../../../../input/data/train'\n",
    "test_dir = '../../../../input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af26a58b-6b25-421c-bdbb-878408369fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>male</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>male</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender  age                  path\n",
       "2697   male   19  006956_male_Asian_19\n",
       "2698   male   20  006957_male_Asian_20\n",
       "2699   male   19  006959_male_Asian_19"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "\n",
    "# drop id, race column\n",
    "train_csv = train_csv.drop(['id', 'race'], axis=1)\n",
    "print(len(train_csv))\n",
    "train_csv.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ffb700-b098-487d-8815-297ccc7b4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(mask, gender, age):\n",
    "    if mask.startswith('mask'):\n",
    "        mask = 0\n",
    "    elif mask.startswith('incorrect'):\n",
    "        mask = 1\n",
    "    else:\n",
    "        mask = 2\n",
    "    return (mask, int(gender == \"female\"), min(2, age // 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15356b59-0c77-4136-a4f4-2430dbac4917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mask</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>y</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>../../../../input/data/train/images/006959_mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>../../../../input/data/train/images/006959_mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>../../../../input/data/train/images/006959_mal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mask  gender  age  y                                               path\n",
       "18897     0       0    0  0  ../../../../input/data/train/images/006959_mal...\n",
       "18898     1       0    0  6  ../../../../input/data/train/images/006959_mal...\n",
       "18899     0       0    0  0  ../../../../input/data/train/images/006959_mal..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path to image_path 변경\n",
    "image_dir = os.path.join(train_dir, 'images')\n",
    "\n",
    "train_meta = pd.DataFrame()\n",
    "for i in range(len(train_csv)):\n",
    "    gender, age, path = train_csv.iloc[i]\n",
    "    image_path = os.path.join(image_dir, path)\n",
    "\n",
    "    li = []\n",
    "    for f in os.listdir(image_path):\n",
    "        if not f.startswith('.'):\n",
    "            y = encode_labels(f, gender, age)\n",
    "            ans = 6*y[0] + 3*y[1] + y[2]\n",
    "            li.append((*y, ans, os.path.join(image_path, f)))\n",
    "    li = pd.DataFrame({name: data for name, data in  zip(['mask', 'gender', 'age', 'y', 'path'], zip(*li))})    \n",
    "    train_meta = train_meta.append(li, ignore_index = True) # 뒤에 계속 합쳐두기\n",
    "train_meta.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "511e45f5-4f5b-4fbb-933f-2a759c039185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, X, y, transform = None):\n",
    "        self.image_paths = X\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        images = []\n",
    "        image = Image.open(self.image_paths[index])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return (image, self.target[index])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d22f6540-e1d0-4de7-901a-2f894ccac0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test dataset 나누기\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(train_meta.path, train_meta.y, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_ds = TrainDataset(X_train, y_train, transform)\n",
    "eval_ds = TrainDataset(X_eval, y_eval, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2d53dcc-4a32-4b84-a5f3-195eefd0efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5002f32-9f05-4c3c-97c0-af625e5044b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "def show_images(data, n = 5, rows=1, cols=7):\n",
    "    plt.figure(figsize=(20,14))\n",
    "\n",
    "    k = 1\n",
    "    for im, *ans in data[:n]:\n",
    "        plt.subplot(rows, cols, k)\n",
    "        plt.imshow(to_pil_image(im))\n",
    "        plt.title(ans, fontsize = 16)\n",
    "        plt.axis('off')\n",
    "        k += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451e7f5-329f-4fa3-9d99-09d23013ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in range(1):\n",
    "    data = next(iter(loader))\n",
    "    images.append(data)\n",
    "# print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd199acc-449d-41c9-a406-928319dbbddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "\n",
    "for i, x in enumerate(['mask', 'gender', 'age', 'y']):\n",
    "    data = train_meta[x].value_counts()\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.bar(data.index, data.values, tick_label=data.index)\n",
    "    plt.title(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-organizer",
   "metadata": {},
   "source": [
    "## 1. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e28d5ffe-1c65-4db3-b02d-35e9fab0a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a58ea8ac-4120-4e19-a847-823605bf0a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Loss: 9.1226\n",
      "epoch: 0 Loss: 8.3045\n",
      "epoch: 0 Loss: 8.6941\n",
      "epoch: 0 Loss: 8.6645\n",
      "epoch: 0 Loss: 8.1749\n",
      "epoch: 0 Loss: 8.7959\n",
      "epoch: 0 Loss: 8.3528\n",
      "epoch: 0 Loss: 8.8579\n",
      "epoch: 0 Loss: 9.2015\n",
      "epoch: 0 Loss: 8.6481\n",
      "epoch: 0 Loss: 8.6591\n",
      "epoch: 0 Loss: 8.4867\n",
      "epoch: 0 Loss: 8.5943\n",
      "epoch: 0 Loss: 8.4682\n",
      "epoch: 0 Loss: 8.7490\n",
      "epoch: 0 Loss: 8.8278\n",
      "epoch: 0 Loss: 8.8945\n",
      "epoch: 0 Loss: 8.3643\n",
      "epoch: 0 Loss: 8.3458\n",
      "epoch: 0 Loss: 8.5843\n",
      "epoch: 0 Loss: 8.5503\n",
      "epoch: 0 Loss: 8.8213\n",
      "epoch: 0 Loss: 8.8923\n",
      "epoch: 0 Loss: 8.4993\n",
      "epoch: 0 Loss: 8.3307\n",
      "epoch: 0 Loss: 8.5983\n",
      "epoch: 0 Loss: 8.7810\n",
      "epoch: 0 Loss: 8.8509\n",
      "epoch: 0 Loss: 8.6065\n",
      "epoch: 0 Loss: 8.4477\n",
      "epoch: 0 Loss: 8.4396\n",
      "epoch: 0 Loss: 8.4661\n",
      "epoch: 0 Loss: 8.4757\n",
      "epoch: 0 Loss: 7.8661\n",
      "epoch: 0 Loss: 8.7623\n",
      "epoch: 0 Loss: 8.7354\n",
      "epoch: 0 Loss: 8.7571\n",
      "epoch: 0 Loss: 8.3666\n",
      "epoch: 0 Loss: 8.4515\n",
      "epoch: 0 Loss: 8.9970\n",
      "epoch: 0 Loss: 8.7260\n",
      "epoch: 0 Loss: 8.4772\n",
      "epoch: 0 Loss: 8.0284\n",
      "epoch: 0 Loss: 9.1089\n",
      "epoch: 0 Loss: 8.8894\n",
      "epoch: 0 Loss: 8.6898\n",
      "epoch: 0 Loss: 8.7787\n",
      "epoch: 0 Loss: 8.6953\n",
      "epoch: 0 Loss: 8.5354\n",
      "epoch: 0 Loss: 8.7750\n",
      "epoch: 0 Loss: 8.3402\n",
      "epoch: 0 Loss: 8.4382\n",
      "epoch: 0 Loss: 9.0202\n",
      "epoch: 0 Loss: 9.0825\n",
      "epoch: 0 Loss: 8.9324\n",
      "epoch: 0 Loss: 8.4381\n",
      "epoch: 0 Loss: 8.7441\n",
      "epoch: 0 Loss: 8.2291\n",
      "epoch: 0 Loss: 8.7319\n",
      "epoch: 0 Loss: 8.6448\n",
      "epoch: 0 Loss: 9.4119\n",
      "epoch: 0 Loss: 7.7958\n",
      "epoch: 0 Loss: 8.2448\n",
      "epoch: 1 Loss: 8.6890\n",
      "epoch: 1 Loss: 8.2625\n",
      "epoch: 1 Loss: 8.6502\n",
      "epoch: 1 Loss: 8.6882\n",
      "epoch: 1 Loss: 8.6838\n",
      "epoch: 1 Loss: 8.6229\n",
      "epoch: 1 Loss: 8.9855\n",
      "epoch: 1 Loss: 8.6462\n",
      "epoch: 1 Loss: 8.5438\n",
      "epoch: 1 Loss: 8.7848\n",
      "epoch: 1 Loss: 8.0137\n",
      "epoch: 1 Loss: 8.2453\n",
      "epoch: 1 Loss: 8.5041\n",
      "epoch: 1 Loss: 8.3488\n",
      "epoch: 1 Loss: 8.5509\n",
      "epoch: 1 Loss: 8.6822\n",
      "epoch: 1 Loss: 8.3402\n",
      "epoch: 1 Loss: 8.4138\n",
      "epoch: 1 Loss: 8.8828\n",
      "epoch: 1 Loss: 8.7861\n",
      "epoch: 1 Loss: 8.2093\n",
      "epoch: 1 Loss: 8.4032\n",
      "epoch: 1 Loss: 8.5267\n",
      "epoch: 1 Loss: 8.3515\n",
      "epoch: 1 Loss: 8.4611\n",
      "epoch: 1 Loss: 8.9724\n",
      "epoch: 1 Loss: 8.5575\n",
      "epoch: 1 Loss: 8.5508\n",
      "epoch: 1 Loss: 8.5794\n",
      "epoch: 1 Loss: 9.1875\n",
      "epoch: 1 Loss: 8.5069\n",
      "epoch: 1 Loss: 8.4508\n",
      "epoch: 1 Loss: 8.3306\n",
      "epoch: 1 Loss: 8.4139\n",
      "epoch: 1 Loss: 8.6054\n",
      "epoch: 1 Loss: 8.5407\n",
      "epoch: 1 Loss: 8.9493\n",
      "epoch: 1 Loss: 8.7646\n",
      "epoch: 1 Loss: 8.8389\n",
      "epoch: 1 Loss: 8.5552\n",
      "epoch: 1 Loss: 8.7467\n",
      "epoch: 1 Loss: 8.9145\n",
      "epoch: 1 Loss: 8.3724\n",
      "epoch: 1 Loss: 8.6612\n",
      "epoch: 1 Loss: 8.4555\n",
      "epoch: 1 Loss: 8.7243\n",
      "epoch: 1 Loss: 8.8598\n",
      "epoch: 1 Loss: 8.7864\n",
      "epoch: 1 Loss: 8.1570\n",
      "epoch: 1 Loss: 8.8784\n",
      "epoch: 1 Loss: 8.5741\n",
      "epoch: 1 Loss: 8.6504\n",
      "epoch: 1 Loss: 8.3500\n",
      "epoch: 1 Loss: 8.7966\n",
      "epoch: 1 Loss: 8.6424\n",
      "epoch: 1 Loss: 8.7537\n",
      "epoch: 1 Loss: 8.3598\n",
      "epoch: 1 Loss: 8.3151\n",
      "epoch: 1 Loss: 8.6764\n",
      "epoch: 1 Loss: 8.9410\n",
      "epoch: 1 Loss: 8.8889\n",
      "epoch: 1 Loss: 8.7705\n",
      "epoch: 1 Loss: 8.4007\n",
      "epoch: 2 Loss: 7.6432\n",
      "epoch: 2 Loss: 8.9283\n",
      "epoch: 2 Loss: 8.7714\n",
      "epoch: 2 Loss: 8.2139\n",
      "epoch: 2 Loss: 8.3291\n",
      "epoch: 2 Loss: 8.5731\n",
      "epoch: 2 Loss: 8.5729\n",
      "epoch: 2 Loss: 8.7130\n",
      "epoch: 2 Loss: 8.8000\n",
      "epoch: 2 Loss: 8.8038\n",
      "epoch: 2 Loss: 8.4057\n",
      "epoch: 2 Loss: 8.3122\n",
      "epoch: 2 Loss: 8.7895\n",
      "epoch: 2 Loss: 8.7447\n",
      "epoch: 2 Loss: 8.7901\n",
      "epoch: 2 Loss: 8.7804\n",
      "epoch: 2 Loss: 8.6066\n",
      "epoch: 2 Loss: 8.5627\n",
      "epoch: 2 Loss: 8.5334\n",
      "epoch: 2 Loss: 9.1553\n",
      "epoch: 2 Loss: 8.0485\n",
      "epoch: 2 Loss: 8.1524\n",
      "epoch: 2 Loss: 8.4222\n",
      "epoch: 2 Loss: 8.3696\n",
      "epoch: 2 Loss: 8.5520\n",
      "epoch: 2 Loss: 8.8862\n",
      "epoch: 2 Loss: 8.9656\n",
      "epoch: 2 Loss: 8.6134\n",
      "epoch: 2 Loss: 8.1730\n",
      "epoch: 2 Loss: 8.6024\n",
      "epoch: 2 Loss: 8.8338\n",
      "epoch: 2 Loss: 8.1552\n",
      "epoch: 2 Loss: 8.5650\n",
      "epoch: 2 Loss: 8.9742\n",
      "epoch: 2 Loss: 8.4023\n",
      "epoch: 2 Loss: 8.8021\n",
      "epoch: 2 Loss: 8.7118\n",
      "epoch: 2 Loss: 8.7344\n",
      "epoch: 2 Loss: 8.3834\n",
      "epoch: 2 Loss: 8.6369\n",
      "epoch: 2 Loss: 8.2784\n",
      "epoch: 2 Loss: 8.7770\n",
      "epoch: 2 Loss: 8.4122\n",
      "epoch: 2 Loss: 8.4689\n",
      "epoch: 2 Loss: 8.4670\n",
      "epoch: 2 Loss: 9.1380\n",
      "epoch: 2 Loss: 8.5329\n",
      "epoch: 2 Loss: 8.8841\n",
      "epoch: 2 Loss: 8.3678\n",
      "epoch: 2 Loss: 9.1399\n",
      "epoch: 2 Loss: 8.7440\n",
      "epoch: 2 Loss: 8.8698\n",
      "epoch: 2 Loss: 9.0202\n",
      "epoch: 2 Loss: 8.1881\n",
      "epoch: 2 Loss: 8.8470\n",
      "epoch: 2 Loss: 7.8466\n",
      "epoch: 2 Loss: 8.8720\n",
      "epoch: 2 Loss: 7.8761\n",
      "epoch: 2 Loss: 8.7218\n",
      "epoch: 2 Loss: 8.3932\n",
      "epoch: 2 Loss: 8.4860\n",
      "epoch: 2 Loss: 8.3272\n",
      "epoch: 2 Loss: 9.1540\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet34(pretrained=True).to(device)\n",
    "loss_fn = FocalLoss()\n",
    "optm = torch.optim.RMSprop(resnet.parameters(), lr=0.05)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for i, (img, label) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optm.zero_grad()\n",
    "        y_pred = model.forward(img)\n",
    "        loss = loss_fn(y_pred, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optm.step()\n",
    "        \n",
    "        if i % 30 == 0:\n",
    "            print(\"epoch: {} Loss: {:.4f}\".format(epoch, loss.data))\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "471690ac-bcda-4196-a817-461e8b66e38e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1646",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1646",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-517f0a768a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ea2608162a32>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1646"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    total_pred = []\n",
    "    for i, (img, label) in enumerate(eval_loader):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        y_pred = model.forward(img)\n",
    "        total_pred.append(y_pred)\n",
    "        print('first')\n",
    "\n",
    "print(\"F1 Loss: {:.4f}\".format( np.mean(f1_score(eval_ds.target, total_pred, average=None)) ))\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 2. Test Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = MyModel(num_classes=18).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "# submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "submission.to_csv(os.path.join(test_dir, 'resnet34_focal_rmsprop3.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e90567-d3e2-46e0-9792-aa3d7cdbdbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
